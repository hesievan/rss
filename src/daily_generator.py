#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import logging
from datetime import datetime
from typing import List, Dict, Any
from collections import defaultdict
from word_group_parser import WordGroupParser

class DailyGenerator:
    """æ—¥æŠ¥ç”Ÿæˆæ¨¡å—"""
    
    def __init__(self):
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def generate_daily_report(self, articles: List[Dict[str, Any]], 
                            max_items: int = 50) -> Dict[str, Any]:
        """ç”Ÿæˆæ—¥æŠ¥å†…å®¹"""
        if not articles:
            return self._generate_empty_report()
        
        # é™åˆ¶æ–‡ç« æ•°é‡
        articles = articles[:max_items]
        
        # æŒ‰æ¥æºåˆ†ç»„
        articles_by_source = self._group_by_source(articles)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'title': self._generate_title(),
            'summary': self._generate_summary(articles),
            'sections': self._generate_sections(articles_by_source),
            'statistics': self._generate_statistics(articles),
            'generated_at': datetime.now().isoformat()
        }
        
        self.logger.info(f"ç”Ÿæˆæ—¥æŠ¥å®Œæˆï¼ŒåŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
        return report
    
    def _generate_title(self) -> str:
        """ç”Ÿæˆæ—¥æŠ¥æ ‡é¢˜"""
        today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        return f"ğŸ“° ç§‘æŠ€æ—¥æŠ¥ - {today}"
    
    def _generate_summary(self, articles: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ—¥æŠ¥æ‘˜è¦"""
        total_articles = len(articles)
        sources = set(article.get('source', '') for article in articles)
        
        summary = f"ä»Šæ—¥å…±ç­›é€‰å‡º {total_articles} ç¯‡é‡è¦èµ„è®¯ï¼Œ"
        summary += f"æ¥è‡ª {len(sources)} ä¸ªä¿¡æ¯æºã€‚"
        
        if total_articles > 0:
            # ç»Ÿè®¡çƒ­é—¨å…³é”®è¯
            keyword_stats = self._analyze_keywords(articles)
            if keyword_stats:
                top_keywords = list(keyword_stats.items())[:3]
                keyword_str = "ã€".join([f"{kw}({count})" for kw, count in top_keywords])
                summary += f" çƒ­é—¨è¯é¢˜ï¼š{keyword_str}ã€‚"
        
        return summary
    
    def _generate_sections(self, articles_by_source: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ—¥æŠ¥ç« èŠ‚"""
        sections = []
        
        for source_name, articles in articles_by_source.items():
            section = {
                'title': f"ğŸ“Š {source_name}",
                'articles': []
            }
            
            for article in articles:
                article_item = {
                    'title': article.get('title', ''),
                    'link': article.get('link', ''),
                    'published': article.get('published').strftime('%H:%M') if article.get('published') else '',
                    'summary': self._truncate_summary(article.get('summary', ''), 100)
                }
                section['articles'].append(article_item)
            
            sections.append(section)
        
        return sections
    
    def _generate_statistics(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        sources = defaultdict(int)
        hours = defaultdict(int)
        
        for article in articles:
            sources[article.get('source', '')] += 1
            published = article.get('published')
            if published:
                hour = published.hour
                hours[f"{hour:02d}:00"] += 1
        
        return {
            'total_articles': len(articles),
            'source_distribution': dict(sources),
            'hourly_distribution': dict(hours),
            'top_sources': sorted(sources.items(), key=lambda x: x[1], reverse=True)[:3]
        }
    
    def _group_by_source(self, articles: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰æ¥æºåˆ†ç»„æ–‡ç« """
        grouped = defaultdict(list)
        for article in articles:
            grouped[article['source']].append(article)
        return dict(grouped)
    
    def _analyze_keywords(self, articles: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æå…³é”®è¯é¢‘ç‡"""
        keyword_count = defaultdict(int)
        
        # å¸¸è§ç§‘æŠ€å…³é”®è¯
        tech_keywords = [
            'AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'ChatGPT', 'å¤§æ¨¡å‹', 'ç§‘æŠ€', 'åˆ›æ–°',
            'åˆ›ä¸š', 'æŠ•èµ„', 'èèµ„', 'IPO', 'ä¸Šå¸‚', 'æ”¶è´­', 'åˆå¹¶', 'è£å‘˜',
            'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æ–°èƒ½æº', 'ç”µåŠ¨è½¦', 'å…ƒå®‡å®™', 'Web3', 'åŒºå—é“¾'
        ]
        
        for article in articles:
            title = article['title'].lower()
            summary = article.get('summary', '').lower()
            content = f"{title} {summary}"
            
            for keyword in tech_keywords:
                if keyword.lower() in content:
                    keyword_count[keyword] += 1
        
        return dict(sorted(keyword_count.items(), key=lambda x: x[1], reverse=True))
    
    def _truncate_summary(self, summary: str, max_length: int) -> str:
        """æˆªæ–­æ‘˜è¦"""
        if len(summary) <= max_length:
            return summary
        return summary[:max_length] + "..."
    
    def _generate_empty_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºæŠ¥å‘Š"""
        return {
            'title': self._generate_title(),
            'summary': "ä»Šæ—¥æš‚æ— é‡è¦èµ„è®¯ã€‚",
            'sections': [],
            'statistics': {
                'total_articles': 0,
                'source_distribution': {},
                'hourly_distribution': {},
                'top_sources': []
            },
            'generated_at': datetime.now().isoformat()
        }
    
    def format_for_feishu(self, report: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸ºé£ä¹¦æ¶ˆæ¯æ ¼å¼"""
        content = f"# {report['title']}\n\n"
        content += f"## ğŸ“‹ ä»Šæ—¥æ‘˜è¦\n{report['summary']}\n\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats = report['statistics']
        content += f"## ğŸ“Š æ•°æ®ç»Ÿè®¡\n"
        content += f"- æ€»æ–‡ç« æ•°ï¼š{stats['total_articles']} ç¯‡\n"
        if stats['top_sources']:
            content += f"- ä¸»è¦æ¥æºï¼š{', '.join([f'{source}({count})' for source, count in stats['top_sources']])}\n"
        content += "\n"
        
        # æ·»åŠ å„ç« èŠ‚å†…å®¹
        for section in report['sections']:
            content += f"## {section['title']}\n"
            for article in section['articles']:
                content += f"- **{article['title']}** [{article['published']}]\n"
                content += f"  {article['link']}\n"
                if article['summary']:
                    content += f"  > {article['summary']}\n"
                content += "\n"
        
        content += f"---\n*ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        
        return content
    
    def format_for_markdown(self, report: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸º Markdown æ ¼å¼"""
        return self.format_for_feishu(report)
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not filename:
            today = datetime.now().strftime('%Y%m%d')
            filename = f"reports/daily_report_{today}.json"
        
        try:
            import os
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return ""
    
    def generate_trendar_style_report(self, group_results: list) -> str:
        """
        ç”Ÿæˆ TrendRadar é£æ ¼çš„åˆ†ç»„ç»Ÿè®¡æ–‡æœ¬ã€‚
        group_results: ContentFilter.filter_by_groups çš„è¾“å‡º
        """
        lines = []
        for idx, group_result in enumerate(group_results, 1):
            group = group_result['group']
            articles = group_result['matched_articles']
            # ç»„æè¿°
            desc = []
            desc += group.get('keywords', [])
            desc += [f"+{w}" for w in group.get('must_keywords', [])]
            desc += [f"!{w}" for w in group.get('exclude_keywords', [])]
            desc_str = 'ã€'.join(desc)
            lines.append(f"ğŸ”¥ {desc_str} : {len(articles)} æ¡\n")
            # ç»„å†…æ–‡ç« ç»Ÿè®¡
            # æŒ‰æ¥æº+æ ‡é¢˜å»é‡+ç»Ÿè®¡å‡ºç°æ¬¡æ•°
            stat_map = {}
            for art in articles:
                key = (art.get('source', ''), art.get('title', ''))
                if key not in stat_map:
                    stat_map[key] = {
                        'article': art,
                        'count': 1,
                        'first_time': art.get('published'),
                        'last_time': art.get('published')
                    }
                else:
                    stat_map[key]['count'] += 1
                    # æ›´æ–°æ—¶é—´èŒƒå›´
                    if art.get('published') < stat_map[key]['first_time']:
                        stat_map[key]['first_time'] = art.get('published')
                    if art.get('published') > stat_map[key]['last_time']:
                        stat_map[key]['last_time'] = art.get('published')
            # æ’åºï¼šå‡ºç°æ¬¡æ•°å¤šã€æ—¶é—´æ–°ä¼˜å…ˆ
            stat_list = sorted(stat_map.values(), key=lambda x: (-x['count'], x['first_time']))
            for i, stat in enumerate(stat_list, 1):
                art = stat['article']
                src = art.get('source', '')
                title = art.get('title', '')
                # æ—¶é—´æ ¼å¼
                ft = stat['first_time'].strftime('%H:%M') if stat['first_time'] else ''
                lt = stat['last_time'].strftime('%H:%M') if stat['last_time'] else ''
                time_str = f"{ft}" if ft == lt else f"{ft} ~ {lt}"
                count_str = f"({stat['count']}æ¬¡)" if stat['count'] > 1 else ''
                lines.append(f"  {i}. [{src}] {title} - {time_str} {count_str}")
            lines.append("")
        return '\n'.join(lines)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    generator = DailyGenerator()
    
    # æ¨¡æ‹Ÿæ–‡ç« æ•°æ®
    test_articles = [
        {
            'title': 'OpenAI å‘å¸ƒ GPT-5 æ¨¡å‹',
            'link': 'https://example.com/1',
            'published': datetime.now(),
            'summary': 'OpenAI å‘å¸ƒäº†æœ€æ–°çš„ GPT-5 æ¨¡å‹ï¼Œæ€§èƒ½å¤§å¹…æå‡',
            'source': '36æ°ª'
        },
        {
            'title': 'ç‰¹æ–¯æ‹‰å‘å¸ƒæ–°æ¬¾ç”µåŠ¨è½¦',
            'link': 'https://example.com/2',
            'published': datetime.now(),
            'summary': 'ç‰¹æ–¯æ‹‰å‘å¸ƒäº†å…¨æ–°çš„ç”µåŠ¨è½¦äº§å“çº¿',
            'source': 'è™å—…ç½‘'
        }
    ]
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generator.generate_daily_report(test_articles)
    
    # æ ¼å¼åŒ–ä¸ºé£ä¹¦æ¶ˆæ¯
    feishu_content = generator.format_for_feishu(report)
    print("é£ä¹¦æ¶ˆæ¯æ ¼å¼:")
    print(feishu_content)
    
    # ä¿å­˜æŠ¥å‘Š
    generator.save_report(report) 